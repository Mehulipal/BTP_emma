# -*- coding: utf-8 -*-
"""1901CS78_BTP_emma.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14HiuNXjuj4YJ-FtXeuaIOgekYdWQT6MT
"""

import os
import math
import torch
import random
import logging 
import numpy as np

from pprint import pformat
from argparse import ArgumentParser
from tokenizer_emma import EmmaDataset
from torch.utils.data import DataLoader
from ignite.engine import Engine, Events
from ignite.handlers import ModelCheckpoint
from ignite.contrib.handlers import ProgressBar, PiecewiseLinear
from ignite.metrics import Loss, MetricsLambda, RunningAverage
from ignite.contrib.handlers.tensorboard_logger import TensorboardLogger, OutputHandler, OptimizerParamsHandler
from transformers import (OpenAIGPTLMHeadModel, GPT2LMHeadModel, AdamW, GPT2Tokenizer, BertTokenizer, WEIGHTS_NAME, CONFIG_NAME)

def setup_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True


setup_seed(2019)

def fn_scalar(scalar, args): return scalar

# Create training and validation data loaders to load the data in batches
def data_loaders(args, tokenizer, logger):
    train_dataset = EmmaDataset(tokenizer, data_path=args.train_path) # populate tokenizer and data_path arguments of EmmaDataset class
    valid_dataset = EmmaDataset(tokenizer, data_path=args.valid_path) # populate tokenizer and data_path arguments of EmmaDataset class

    train_loader = DataLoader(train_dataset,
                              collate_fn=train_dataset.collate,
                              pin_memory=(args.device == "cuda"),
                              num_workers=args.num_workers,
                              batch_size=args.train_batch_size,
                              shuffle=False)
    valid_loader = DataLoader(valid_dataset,
                              collate_fn=valid_dataset.collate,
                              pin_memory=(args.device == "cuda"),
                              num_workers=args.num_workers,
                              batch_size=args.valid_batch_size,
                              shuffle=False)
    return train_loader, valid_loader

def train():
    # Arguments inputted by user
    parser = ArgumentParser()
    parser.add_argument("--train_path", type=str, default="data/train.txt", help="Path of the train dataset for dist dataset. ")
    parser.add_argument("--valid_path", type=str, default="data/dev.txt", help="Path of the valid dataset for dist dataset. ")
    parser.add_argument("--train_batch_size", type=int, default=16, help="Batch size for training")
    parser.add_argument("--valid_batch_size", type=int, default=16, help="Batch size for validation")
    parser.add_argument("--lr", type=float, default=1e-3, help="Learning rate")
    parser.add_argument("--n_epochs", type=int, default=3, help="Number of training epochs")
    parser.add_argument('--gpt2', action='store_true', help="use gpt2")
    parser.add_argument("--model_checkpoint", type=str, default="model_checkpoint", help="Path or URL of the model")
    parser.add_argument("--num_workers", type=int, default=8, help="Number of subprocesses for data loading")
    parser.add_argument("--valid_steps", type=int, default=100, help="Perfom validation every X steps")
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu", help="Device (cuda or cpu)")
    parser.add_argument("--scheduler", type=str, default="linear", choices=['noam', 'linear'], help="method of optim")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=64, help="Accumulate gradients on several steps")
    parser.add_argument("--max_norm", type=float, default=1.0, help="Clipping gradient norm")
    parser.add_argument("--eval_before_start", action='store_true', help="If true start with a first evaluation before training")  
    args = parser.parse_args()  # Parsing arguments

    '''
        If we are using GPT-2, model class is GPT2LMHeadModel.
        For other models like BERT, model class is OpenAIGPTLMHeadModel.
    '''
    model_class = OpenAIGPTLMHeadModel if not args.gpt2 else GPT2LMHeadModel
    model = model_class.from_pretrained(args.model_checkpoint)
    print("Model:", model_class)

    '''
        If we are using GPT-2, tokenizer class is GPT2Tokenizer.
        For other models like BERT, tokenizer class is BertTokenizer.
    '''
    tokenizer_class = BertTokenizer if not args.gpt2 else GPT2Tokenizer
    tokenizer = tokenizer_class.from_pretrained(args.model_checkpoint, do_lower_case=True)
    print("Tokenizer:", tokenizer_class)

    # Add special tokens to vocabulary
    special_tokens_dict = {'additional_special_tokens': ['[CLS]', '[SEP]', '[None]']}
    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)
    model.resize_token_embeddings(len(tokenizer))

    # Save and load models across devices (CUDA or CPU)
    # User can set device according to requirements via arguments
    model.to(args.device)

    '''
        We use AdamW (Adaptive Moment Estimation) optimizer. 
        It is an algorithm for optimization technique for gradient descent.
        AdamW optimizer involves a combination of two gradient descent methodologies:
        (a) Momentum (b) Root Mean Square Propagation (RMSP)
    '''
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)
    print('\nOptimizer:',optimizer)

    # Create training and validation data loaders to load the data in batches
    # train_batch_size and valid_batch_size are taken as user inputs via arguments
    loader_class = data_loaders
    logger = logging.getLogger(__file__)
    train_loader, val_loader = loader_class(args, tokenizer, logger)    

    '''
        Training function and trainer.
        In 1 epoch, the model is trained/ fine-tuned on a batch of data points from the emma dataset. 
        Loss is calculated after every epoch.
    '''
    def update(engine, batch):       
        '''
            input_ids:        Indices of input sequence tokens in the vocabulary. 
                              For eg, [CLS] My name is Mehuli. [SEP] I am in BTech. [SEP], which can be interpreted as
                              [101, 8, 5, 6, 7, 3, 102, 9, 2, 1, 4, 10, 102]

            token_type_ids:   Same as the segment ids, which differentiates sentence 1 and 2 in 2-sentence tasks. 
                              In [0, 0, 0, 1, 1, 1, 1], '0' represent sentence 1 & '1' represents sentence 2.

            lm_labels:        Language modeling labels. Indices are selected in [-100, 0, ..., config.vocab_size].
                              All labels set to -100 are ignored (masked), the loss is only computed for labels in 
                              [0, ..., config.vocab_size]
        '''
        input_ids, token_type_ids, lm_labels = tuple(input_tensor.to(args.device) for input_tensor in batch)
        
        # Reference: https://discuss.huggingface.co/t/use-of-input-ids-token-type-ids-and-lm-labels-in-bert-language-model/1202
        for label in lm_labels:
            for i in range(len(label)):
                if label[i] == -1: label[i] = -100
        
        model.train() # training the model

        # Loss function / Cost function / Error function
        lm_loss = model(input_ids, labels=lm_labels, token_type_ids=token_type_ids).loss
        loss = lm_loss / args.gradient_accumulation_steps 
        '''
            backward() method in Pytorch is used to calculate the error derivative/ 
            gradient during backpropagation in neural network.
        '''
        loss.backward() 
        '''
            Gradient Clipping is a method where the error derivative is changed or clipped to a threshold 
            during backpropagation through the network, and the clipped gradients are used to update the weights.
        '''
        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_norm) # gradient clipping
        
        '''
            In PyTorch, for every mini-batch during the training phase, we typically want to explicitly set 
            the gradients to zero before starting to do backpropragation (i.e., updating the Weights and biases) 
            because PyTorch accumulates the gradients on subsequent backward passes. This accumulating behaviour 
            is convenient while training RNNs or when we want to compute the gradient of the loss summed over 
            multiple mini-batches. So, the default action has been set to accumulate (i.e. sum) the gradients 
            on every loss.backward() call. Because of this, when we start training loop, ideally we should
            zero out the gradients so that you do the parameter update correctly. Otherwise, the gradient would be
            a combination of the old gradient, which you have already used to update your model parameters, and 
            the newly-computed gradient. It would therefore point in some other direction than the intended 
            direction towards the minimum (or maximum, in case of maximization objectives).
        '''
        if engine.state.iteration % args.gradient_accumulation_steps == 0:
            optimizer.step() # performs a single optimization step (parameter update)
            optimizer.zero_grad()
    
        '''
            When we define the optimizer, we have the option of partitioning the model parameters into different groups, 
            called param groups. Each param group can have different optimizer settings. For eg, one group of parameters 
            could have learning rate of 0.1 and another could have learning rate of 0.01.
        '''
        return loss.item(), optimizer.param_groups[0]['lr']
    
    trainer = Engine(update)  # training

    # Evaluation function and evaluator (evaluator output is the input of the metrics)
    def inference(engine, batch):
        model.eval()
        
        with torch.no_grad():
            input_ids, token_type_ids, lm_labels = tuple(input_tensor.to(args.device) for input_tensor in batch)
            
            '''
                In context of deep learning the logits layer means the layer that feeds in to softmax (or other such 
                normalization). The output of the softmax are the probabilities for the classification task and its 
                input is logits layer. The logits layer typically produces values from -infinity to +infinity and the 
                softmax layer transforms it to values from 0 to 1.
            '''
            lm_logits = model(input_ids, token_type_ids=token_type_ids).logits
            lm_logits_flat_shifted = lm_logits[..., :-1, :].contiguous().view(-1, lm_logits.size(-1))
            lm_labels_flat_shifted = lm_labels[..., 1:].contiguous().view(-1)
            return lm_logits_flat_shifted, lm_labels_flat_shifted

    evaluator = Engine(inference) # evaluation

    # Attach evaluation to trainer: we evaluate when we start the training and at the end of each epoch
    trainer.add_event_handler(Events.EPOCH_COMPLETED, lambda _: evaluator.run(val_loader))
    if args.n_epochs < 1:
        trainer.add_event_handler(Events.COMPLETED, lambda _: evaluator.run(val_loader))
    if args.eval_before_start:
        trainer.add_event_handler(Events.STARTED, lambda _: evaluator.run(val_loader))

    # Evaluation during training
    @trainer.on(Events.ITERATION_STARTED)
    def log_iterations(engine):
        print("\n\nBatch:",engine.state.iteration)
        if engine.state.iteration % args.valid_steps == 0:
            evaluator.run(val_loader)

    '''
        Initially, sets the learning rate to args.lr (i.e., user inputted learning rate) in the 0th iteration, 
        then decreases linearly to 0.0 until the end of the iterations, given by (args.n_epochs * len(train_loader) 
    '''
    scheduler = PiecewiseLinear(optimizer, "lr", [(0, args.lr), (args.n_epochs * len(train_loader), 0.0)])
    trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)

    # Prepare metrics
    RunningAverage(output_transform=lambda x: x[0]).attach(trainer, "loss")
    RunningAverage(output_transform=lambda x: x[1]).attach(trainer, "lr")

    '''
        NLL Loss: Negative log-likelihood minimization is a proxy problem to the problem of maximum likelihood estimation.
        Cross-entropy and negative log-likelihood are closely related mathematical formulations.
        The essential part of computing the negative log-likelihood is to “sum up the correct log probabilities.”
        After converting logits into probabilities by passing through a softmax layer, the purpose of NLL is to take the 
        output probabilities and measure the distance from the truth values. 
    '''
    metrics = {"negative_log_likelihood_loss": Loss(torch.nn.CrossEntropyLoss(ignore_index=-1), output_transform=lambda x: (x[0], x[1]))}
    metrics["average_ppl"] = MetricsLambda(math.exp, metrics["negative_log_likelihood_loss"])
    for name, metric in metrics.items():
        metric.attach(evaluator, name)

    pbar = ProgressBar(persist=True, mininterval=2)
    pbar.attach(trainer, metric_names=["loss", "lr"])
    evaluator.add_event_handler(Events.COMPLETED, lambda _: pbar.log_message("\nValidation: %s" % pformat(evaluator.state.metrics)))

    tb_logger = TensorboardLogger(log_dir=None)
    tb_logger.attach(trainer, log_handler=OutputHandler(tag="training", metric_names=["loss"]), event_name=Events.ITERATION_COMPLETED)
    tb_logger.attach(trainer, log_handler=OptimizerParamsHandler(optimizer), event_name=Events.ITERATION_STARTED)
    tb_logger.attach(evaluator, log_handler=OutputHandler(tag="validation", metric_names=list(metrics.keys())), event_name=Events.EPOCH_COMPLETED)

    checkpoint_handler = ModelCheckpoint(tb_logger.writer.log_dir, 'checkpoint', save_interval=1, n_saved=3)
    # save model after evaluation
    evaluator.add_event_handler(Events.EPOCH_COMPLETED, checkpoint_handler, {'mymodel': getattr(model, 'module', model)})
    trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpoint_handler, {'mymodel': getattr(model, 'module', model)})  # "getattr" take care of distributed encapsulation

    torch.save(args, tb_logger.writer.log_dir + '/model_training_args.bin')
    getattr(model, 'module', model).config.to_json_file(os.path.join(tb_logger.writer.log_dir, CONFIG_NAME))
    tokenizer.save_vocabulary(tb_logger.writer.log_dir)

    # Run the training
    trainer.run(train_loader, max_epochs=args.n_epochs)
    
if __name__ == "__main__":
    train()