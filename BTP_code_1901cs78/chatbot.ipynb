{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Run training and validation for EMMA"],"metadata":{"id":"E5Yz_CaotvXQ"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"A0SzcgKuH8XH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2bca7145-5579-4e3f-f65b-2948b0df3658"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["%cd drive/MyDrive/emma/code/"],"metadata":{"id":"D85lCC3sIGNP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d2b31127-820b-4007-9ef4-2341c0299ade"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/emma/code\n"]}]},{"cell_type":"code","source":["!pip install pytorch-ignite\n","!pip install transformers"],"metadata":{"id":"5D2pq4bhNcqC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e40a4fea-c366-4c9a-d617-41a6eb85369d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch-ignite\n","  Downloading pytorch_ignite-0.4.10-py3-none-any.whl (264 kB)\n","\u001b[K     |████████████████████████████████| 264 kB 26.4 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (21.3)\n","Requirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.12.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytorch-ignite) (3.0.9)\n","Installing collected packages: pytorch-ignite\n","Successfully installed pytorch-ignite-0.4.10\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n","\u001b[K     |████████████████████████████████| 5.5 MB 15.6 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 49.8 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 86.0 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.24.0\n"]}]},{"cell_type":"code","source":["!python train_moban.py --pretrained --model_checkpoint \"bert-base-uncased\" --train_path ../data/raw/english_demo.json --valid_path ../data/raw/english_demo.json --scheduler linear --num_workers 2 --valid_steps 3 --lr 0.001 "],"metadata":{"id":"u-oY4nG1Ioj_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"22b90870-8e38-4d84-d859-be338d9744c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Prepare tokenizer, pretrained model and optimizer - add special tokens for fine-tuning\n","\n","Model: <class 'transformers.models.openai.modeling_openai.OpenAIGPTLMHeadModel'>\n","Config: <class 'transformers.models.openai.configuration_openai.OpenAIGPTConfig'>\n","Tokenizer: <class 'transformers.models.bert.tokenization_bert.BertTokenizer'>\n","\n","Using pretrained model\n","\n","Downloading: 100% 232k/232k [00:00<00:00, 1.24MB/s]\n","Downloading: 100% 28.0/28.0 [00:00<00:00, 16.1kB/s]\n","Downloading: 100% 570/570 [00:00<00:00, 447kB/s]\n","You are using a model of type bert to instantiate a model of type openai-gpt. This is not supported for all configurations of models and can yield errors.\n","Downloading: 100% 440M/440M [00:07<00:00, 57.0MB/s]\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing OpenAIGPTLMHeadModel: ['bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'cls.predictions.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.output.dense.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight']\n","- This IS expected if you are initializing OpenAIGPTLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing OpenAIGPTLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of OpenAIGPTLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['h.3.attn.c_proj.bias', 'h.10.ln_1.bias', 'h.1.mlp.c_proj.weight', 'h.3.mlp.c_fc.bias', 'h.1.mlp.c_proj.bias', 'h.9.mlp.c_proj.weight', 'h.11.mlp.c_fc.weight', 'h.1.mlp.c_fc.bias', 'h.7.ln_1.bias', 'h.1.attn.c_proj.bias', 'h.0.ln_2.weight', 'h.8.attn.c_proj.bias', 'h.8.ln_1.weight', 'h.11.attn.c_proj.bias', 'h.9.ln_1.weight', 'h.2.ln_1.bias', 'h.11.ln_2.bias', 'h.6.attn.c_proj.weight', 'h.0.attn.c_proj.bias', 'h.4.attn.c_proj.weight', 'h.2.ln_2.bias', 'h.9.attn.c_proj.weight', 'h.1.ln_2.bias', 'h.2.attn.c_proj.bias', 'h.7.ln_2.weight', 'h.8.mlp.c_proj.bias', 'h.1.attn.c_attn.bias', 'h.0.ln_1.bias', 'h.1.ln_2.weight', 'tokens_embed.weight', 'h.0.mlp.c_fc.weight', 'h.7.mlp.c_fc.bias', 'h.7.attn.c_proj.weight', 'h.6.attn.c_attn.bias', 'h.0.mlp.c_proj.weight', 'h.4.attn.bias', 'h.2.mlp.c_proj.weight', 'h.8.mlp.c_fc.weight', 'h.5.attn.c_proj.weight', 'h.11.attn.bias', 'h.8.ln_2.weight', 'h.6.ln_1.weight', 'h.5.attn.bias', 'h.6.mlp.c_fc.weight', 'h.2.attn.c_attn.bias', 'h.2.attn.c_proj.weight', 'h.2.mlp.c_fc.weight', 'h.8.mlp.c_fc.bias', 'h.11.ln_2.weight', 'h.3.mlp.c_fc.weight', 'h.0.attn.c_attn.bias', 'h.11.mlp.c_fc.bias', 'h.1.mlp.c_fc.weight', 'h.9.attn.c_attn.weight', 'h.3.ln_2.weight', 'h.4.attn.c_proj.bias', 'h.8.attn.c_attn.bias', 'h.0.mlp.c_proj.bias', 'h.7.ln_2.bias', 'h.10.mlp.c_proj.weight', 'h.0.mlp.c_fc.bias', 'h.4.mlp.c_fc.weight', 'h.6.attn.c_attn.weight', 'h.6.ln_2.weight', 'h.6.ln_2.bias', 'h.6.mlp.c_proj.weight', 'h.11.mlp.c_proj.bias', 'h.0.attn.bias', 'h.10.mlp.c_fc.bias', 'h.10.attn.c_attn.bias', 'h.3.ln_1.bias', 'h.3.attn.bias', 'h.4.ln_2.weight', 'h.10.attn.bias', 'h.4.mlp.c_fc.bias', 'h.9.attn.bias', 'h.6.mlp.c_proj.bias', 'h.1.ln_1.bias', 'h.3.attn.c_proj.weight', 'h.8.attn.c_proj.weight', 'h.7.attn.c_attn.bias', 'h.5.ln_1.bias', 'h.1.attn.c_proj.weight', 'h.8.ln_1.bias', 'h.2.attn.bias', 'h.3.attn.c_attn.weight', 'h.6.mlp.c_fc.bias', 'h.9.ln_1.bias', 'h.11.attn.c_proj.weight', 'h.3.ln_2.bias', 'h.2.mlp.c_proj.bias', 'h.10.attn.c_attn.weight', 'h.6.ln_1.bias', 'h.7.attn.c_attn.weight', 'h.9.mlp.c_proj.bias', 'h.0.ln_2.bias', 'h.2.mlp.c_fc.bias', 'h.4.ln_2.bias', 'positions_embed.weight', 'h.9.attn.c_attn.bias', 'h.4.attn.c_attn.weight', 'h.9.attn.c_proj.bias', 'h.5.mlp.c_proj.bias', 'lm_head.weight', 'h.1.ln_1.weight', 'h.10.mlp.c_fc.weight', 'h.9.ln_2.weight', 'h.8.mlp.c_proj.weight', 'h.8.attn.bias', 'h.2.ln_2.weight', 'h.2.ln_1.weight', 'h.5.attn.c_proj.bias', 'h.7.attn.bias', 'h.5.ln_2.bias', 'h.9.mlp.c_fc.bias', 'h.11.ln_1.weight', 'h.10.ln_2.bias', 'h.9.ln_2.bias', 'h.1.attn.c_attn.weight', 'h.11.attn.c_attn.weight', 'h.5.ln_1.weight', 'h.5.attn.c_attn.weight', 'h.5.mlp.c_proj.weight', 'h.1.attn.bias', 'h.11.ln_1.bias', 'h.0.ln_1.weight', 'h.7.mlp.c_proj.weight', 'h.7.mlp.c_proj.bias', 'h.7.ln_1.weight', 'h.0.attn.c_attn.weight', 'h.9.mlp.c_fc.weight', 'h.4.ln_1.bias', 'h.4.mlp.c_proj.bias', 'h.5.mlp.c_fc.weight', 'h.10.attn.c_proj.bias', 'h.8.ln_2.bias', 'h.6.attn.c_proj.bias', 'h.4.mlp.c_proj.weight', 'h.4.attn.c_attn.bias', 'h.10.ln_1.weight', 'h.3.mlp.c_proj.weight', 'h.10.attn.c_proj.weight', 'h.10.ln_2.weight', 'h.6.attn.bias', 'h.5.mlp.c_fc.bias', 'h.2.attn.c_attn.weight', 'h.5.ln_2.weight', 'h.5.attn.c_attn.bias', 'h.7.mlp.c_fc.weight', 'h.4.ln_1.weight', 'h.10.mlp.c_proj.bias', 'h.3.attn.c_attn.bias', 'h.3.ln_1.weight', 'h.8.attn.c_attn.weight', 'h.11.attn.c_attn.bias', 'h.0.attn.c_proj.weight', 'h.7.attn.c_proj.bias', 'h.3.mlp.c_proj.bias', 'h.11.mlp.c_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","\n","Tokenizer Length: 30527 \n","\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","\n","Optimizer: AdamW (\n","Parameter Group 0\n","    betas: (0.9, 0.999)\n","    correct_bias: True\n","    eps: 1e-06\n","    initial_lr: 0.001\n","    lr: 0.001\n","    weight_decay: 0.0\n",") \n","\n","Build train and validation dataloaders\n","\n","/usr/local/lib/python3.7/dist-packages/ignite/handlers/checkpoint.py:993: UserWarning: Argument save_interval is deprecated and should be None. This argument will be removed in 0.5.0.Please, use events filtering instead, e.g. Events.ITERATION_STARTED(every=1000)\n","  warnings.warn(msg)\n","                                                                            \n","Validation: {'average_nll': 2.5277858681611565,\n"," 'average_ppl': 12.525741767371562,\n"," 'nll': 2.5277858681611565}\n","Epoch [1/3]: 100% 267/267 [23:40<00:00,  5.34s/it, loss=0.0493, lr=0.000729]\n","                                                                            \n","Validation: {'average_nll': 1.8213096610418915,\n"," 'average_ppl': 6.179946787785855,\n"," 'nll': 1.8213096610418915}\n","Epoch [2/3]: 100% 267/267 [23:58<00:00,  5.41s/it, loss=0.0326, lr=0.000395]\n","                                                                           \n","Validation: {'average_nll': 1.7059233358677524,\n"," 'average_ppl': 5.506467639743078,\n"," 'nll': 1.7059233358677524}\n","Epoch [3/3]: 100% 267/267 [25:00<00:00,  5.64s/it, loss=0.0274, lr=6.21e-5]\n"]}]},{"cell_type":"code","source":["!python train_moban.py --pretrained --model_checkpoint \"gpt2\" --train_path ../data/raw/english_demo.json --valid_path ../data/raw/english_demo.json --scheduler linear --num_workers 2 --valid_steps 3 --lr 0.001 --n_epochs 9 --gpt2"],"metadata":{"id":"fC-64Jv5Xu71"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd drive/MyDrive/BTP_code"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E05CWqfkpLNK","outputId":"ef410109-a6f9-4c2d-8ff7-8eb70919b743"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/BTP_code\n"]}]},{"cell_type":"code","source":["!python \"train_validate_emma.py\" --model_checkpoint \"gpt2\" --train_path /data/english_demo.json --valid_path /data/english_demo.json --train_batch_size 200 --valid_batch_size 200 --num_workers 2 --valid_steps 3 --lr 0.001 --n_epochs 5 --gpt2"],"metadata":{"id":"yJV9nQ5LROrz","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9fdf7cbb-61d0-40d4-a4a3-892ec474bd35"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading: 100% 665/665 [00:00<00:00, 699kB/s]\n","Downloading: 100% 548M/548M [00:08<00:00, 61.7MB/s]\n","Model: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n","Downloading: 100% 1.04M/1.04M [00:00<00:00, 31.2MB/s]\n","Downloading: 100% 456k/456k [00:00<00:00, 16.8MB/s]\n","Tokenizer: <class 'transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer'>\n","\n","Optimizer: AdamW (\n","Parameter Group 0\n","    amsgrad: False\n","    betas: (0.9, 0.999)\n","    capturable: False\n","    eps: 1e-08\n","    foreach: None\n","    lr: 0.001\n","    maximize: False\n","    weight_decay: 0.01\n",")\n","/usr/local/lib/python3.7/dist-packages/ignite/handlers/checkpoint.py:993: UserWarning: Argument save_interval is deprecated and should be None. This argument will be removed in 0.5.0.Please, use events filtering instead, e.g. Events.ITERATION_STARTED(every=1000)\n","  warnings.warn(msg)\n","\n","\n","Batch: 1\n","Epoch [1/5]:   5% 1/22 [00:00<?, ?it/s, loss=0.697, lr=0.001]\n","\n","Batch: 2\n","Epoch [1/5]:   9% 2/22 [01:56<38:41, 116.07s/it, loss=0.697, lr=0.001]\n","\n","Batch: 3\n","                                                                      \n","Validation: {'average_ppl': 1.879519952685513e+20,\n"," 'negative_log_likelihood_loss': 46.682718259800644}\n","Epoch [1/5]:  14% 3/22 [16:59<3:03:20, 578.97s/it, loss=0.697, lr=0.000999]\n","\n","Batch: 4\n","Epoch [1/5]:  18% 4/22 [18:57<1:50:34, 368.59s/it, loss=0.697, lr=0.000999]\n","\n","Batch: 5\n","Epoch [1/5]:  23% 5/22 [20:53<1:16:11, 268.93s/it, loss=0.697, lr=0.000998]\n","\n","Batch: 6\n","                                                                           \n","Validation: {'average_ppl': 1.879519952685513e+20,\n"," 'negative_log_likelihood_loss': 46.682718259800644}\n","Epoch [1/5]:  27% 6/22 [36:10<2:14:02, 502.68s/it, loss=0.697, lr=0.000997]\n","\n","Batch: 7\n","Epoch [1/5]:  32% 7/22 [38:14<1:33:26, 373.77s/it, loss=0.697, lr=0.000996]\n","\n","Batch: 8\n","Epoch [1/5]:  36% 8/22 [40:17<1:08:07, 291.93s/it, loss=0.697, lr=0.000995]\n","\n","Batch: 9\n","                                                                           \n","Validation: {'average_ppl': 1.879519952685513e+20,\n"," 'negative_log_likelihood_loss': 46.682718259800644}\n","Epoch [1/5]:  41% 9/22 [55:16<1:45:05, 485.06s/it, loss=0.697, lr=0.000994]\n","\n","Batch: 10\n","Epoch [1/5]:  45% 10/22 [57:14<1:14:06, 370.54s/it, loss=0.698, lr=0.000992]\n","\n","Batch: 11\n","Epoch [1/5]:  50% 11/22 [59:17<53:54, 294.07s/it, loss=0.698, lr=0.000991]  \n","\n","Batch: 12\n","                                                                          \n","Validation: {'average_ppl': 1.879519952685513e+20,\n"," 'negative_log_likelihood_loss': 46.682718259800644}\n","Epoch [1/5]:  55% 12/22 [1:14:18<1:19:56, 479.69s/it, loss=0.698, lr=0.000989]\n","\n","Batch: 13\n","Epoch [1/5]:  59% 13/22 [1:16:14<55:21, 369.05s/it, loss=0.697, lr=0.000987]  \n","\n","Batch: 14\n","Epoch [1/5]:  64% 14/22 [1:18:08<38:53, 291.74s/it, loss=0.698, lr=0.000985]\n","\n","Batch: 15\n","                                                                            \n","Validation: {'average_ppl': 1.879519952685513e+20,\n"," 'negative_log_likelihood_loss': 46.682718259800644}\n","Epoch [1/5]:  68% 15/22 [1:33:43<56:41, 486.00s/it, loss=0.698, lr=0.000982]\n","\n","Batch: 16\n","Epoch [1/5]:  73% 16/22 [1:35:36<37:21, 373.60s/it, loss=0.698, lr=0.00098]\n","\n","Batch: 17\n","Epoch [1/5]:  77% 17/22 [1:37:29<24:36, 295.21s/it, loss=0.698, lr=0.000978]\n","\n","Batch: 18\n","                                                                            \n","Validation: {'average_ppl': 1.879519952685513e+20,\n"," 'negative_log_likelihood_loss': 46.682718259800644}\n","Epoch [1/5]:  82% 18/22 [1:52:36<31:57, 479.27s/it, loss=0.698, lr=0.000975]\n","\n","Batch: 19\n","Epoch [1/5]:  86% 19/22 [1:54:36<18:33, 371.14s/it, loss=0.698, lr=0.000972]\n","\n","Batch: 20\n","Epoch [1/5]:  91% 20/22 [1:56:26<09:45, 292.79s/it, loss=0.698, lr=0.000969]\n","\n","Batch: 21\n","                                                                            \n","Validation: {'average_ppl': 1.879519952685513e+20,\n"," 'negative_log_likelihood_loss': 46.682718259800644}\n","Epoch [1/5]:  95% 21/22 [2:11:50<08:02, 482.40s/it, loss=0.698, lr=0.000966]\n","\n","Batch: 22\n","                                                                            \n","Validation: {'average_ppl': 1.879519952685513e+20,\n"," 'negative_log_likelihood_loss': 46.682718259800644}\n","Epoch [1/5]: 100% 22/22 [2:25:32<00:00, 415.85s/it, loss=0.699, lr=0.000963]\n","\n","\n","Batch: 23\n","Epoch [2/5]:   5% 1/22 [00:00<?, ?it/s, loss=0.708, lr=0.0008]\n","\n","Batch: 24\n","                                                              \n","Validation: {'average_ppl': 1.879519952685513e+20,\n"," 'negative_log_likelihood_loss': 46.682718259800644}\n","Epoch [2/5]:   9% 2/22 [15:16<5:05:23, 916.17s/it, loss=0.708, lr=0.0008]\n","\n","Batch: 25\n","Epoch [2/5]:  14% 3/22 [17:08<2:20:22, 443.31s/it, loss=0.707, lr=0.000799]\n","\n","Batch: 26\n","Epoch [2/5]:  18% 4/22 [19:01<1:27:42, 292.39s/it, loss=0.707, lr=0.000799]\n","\n","Batch: 27\n","                                                                           \n","Validation: {'average_ppl': 1.879519952685513e+20,\n"," 'negative_log_likelihood_loss': 46.682718259800644}\n","Epoch [2/5]:  23% 5/22 [34:09<2:31:44, 535.56s/it, loss=0.707, lr=0.000798]\n","\n","Batch: 28\n","Epoch [2/5]:  27% 6/22 [36:01<1:42:06, 382.90s/it, loss=0.707, lr=0.000797]\n","\n","Batch: 29\n","Epoch [2/5]:  32% 7/22 [38:01<1:13:20, 293.38s/it, loss=0.707, lr=0.000796]\n","\n","Batch: 30\n","                                                                           \n","Validation: {'average_ppl': 1.879519952685513e+20,\n"," 'negative_log_likelihood_loss': 46.682718259800644}\n","Epoch [2/5]:  36% 8/22 [53:26<1:56:37, 499.80s/it, loss=0.707, lr=0.000795]\n","\n","Batch: 31\n","Epoch [2/5]:  41% 9/22 [55:25<1:22:02, 378.67s/it, loss=0.706, lr=0.000794]\n","\n","Batch: 32\n","Epoch [2/5]:  45% 10/22 [57:17<59:05, 295.42s/it, loss=0.706, lr=0.000792] \n","\n","Batch: 33\n","ERROR:ignite.engine.engine.Engine:Engine run is terminating due to exception: \n","ERROR:ignite.engine.engine.Engine:Engine run is terminating due to exception: \n","Traceback (most recent call last):\n","  File \"1901cs78_btp_emma.py\", line 227, in <module>\n","    train()\n","  File \"1901cs78_btp_emma.py\", line 224, in train\n","    trainer.run(train_loader, max_epochs=args.n_epochs)\n","  File \"/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\", line 892, in run\n","    return self._internal_run()\n","  File \"/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\", line 935, in _internal_run\n","    return next(self._internal_run_generator)\n","  File \"/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n","    self._handle_exception(e)\n","  File \"/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n","    raise e\n","  File \"/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n","    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n","  File \"/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\", line 1065, in _run_once_on_dataset_as_gen\n","    self._fire_event(Events.ITERATION_STARTED)\n","  File \"/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\", line 425, in _fire_event\n","    func(*first, *(event_args + others), **kwargs)\n","  File \"1901cs78_btp_emma.py\", line 188, in log_iterations\n","    evaluator.run(val_loader)\n","  File \"/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\", line 892, in run\n","    return self._internal_run()\n","  File \"/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\", line 935, in _internal_run\n","    return next(self._internal_run_generator)\n","  File \"/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\", line 993, in _internal_run_as_gen\n","    self._handle_exception(e)\n","  File \"/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\", line 638, in _handle_exception\n","    raise e\n","  File \"/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\", line 959, in _internal_run_as_gen\n","    epoch_time_taken += yield from self._run_once_on_dataset_as_gen()\n","  File \"/usr/local/lib/python3.7/dist-packages/ignite/engine/engine.py\", line 1068, in _run_once_on_dataset_as_gen\n","    self.state.output = self._process_function(self, self.state.batch)\n","  File \"1901cs78_btp_emma.py\", line 169, in inference\n","    lm_logits = model(input_ids, token_type_ids=token_type_ids).logits\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1059, in forward\n","    return_dict=return_dict,\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 897, in forward\n","    output_attentions=output_attentions,\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 426, in forward\n","    feed_forward_hidden_states = self.mlp(hidden_states)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 355, in forward\n","    hidden_states = self.c_proj(hidden_states)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/transformers/pytorch_utils.py\", line 112, in forward\n","    x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n","KeyboardInterrupt\n","Epoch [2/5]:  45% 10/22 [1:09:29<1:32:39, 463.32s/it, loss=0.706, lr=0.000792]\n"]}]},{"cell_type":"code","source":["!python infer_moban.py --datapath ../data/raw/test.json --out_path test_dialog/output.txt --model_checkpoint  \"bert-base-uncased\" --max_length 20 --min_length 1"],"metadata":{"id":"r1GqpqDrWQ3Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["***How to take model_checkpoint for testing (from training)?***"],"metadata":{"id":"K_huLntElM6g"}}]}